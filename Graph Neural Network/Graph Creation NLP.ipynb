{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import mysql.connector\n",
    "import dgl.data\n",
    "import numpy as np\n",
    "from dgl import save_graphs, load_graphs\n",
    "import torch as th\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import requests\n",
    "import time\n",
    "#from transformers import *\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "#model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "cnx = mysql.connector.connect(user='david', password='daviddung1993',\n",
    "                          host='127.0.0.1',\n",
    "                          database='computervision')\n",
    "cursor = cnx.cursor()\n",
    "headers = {\"x-api-key\": \"M7HSjQNeTfai6l7JUiDZB8XYc85BHnHt3R0NXSEd\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "test_papers = []\n",
    "with open(r'./test_papers.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        test_papers.append(x)\n",
    "in_params_test = ','.join(['%s'] * len(test_papers))\n",
    "\n",
    "train_papers = []\n",
    "with open(r'./train_papers.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        train_papers.append(x)\n",
    "\n",
    "total_papers = train_papers + test_papers\n",
    "in_params_train = ','.join(['%s'] * len(train_papers))\n",
    "\n",
    "val_papers = []\n",
    "with open(r'./val_papers.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        val_papers.append(x)\n",
    "\n",
    "in_params_val = ','.join(['%s'] * len(val_papers))\n",
    "in_params = ','.join(['%s'] * len(train_papers + test_papers))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def remap(x, mapping):\n",
    "    return mapping.get(x, x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "cursor.execute(\"select distinct p2.PaperID from Papers p, authoredBy b, authoredBy b2, Papers p2 where p.PaperID in (%s) and p.PaperID = b.PaperID and b.AuthoredByID = b2.AuthoredByID and b2.PaperID = p2.PaperID and p2.Pub_Year != 2022\" % in_params_test, test_papers)\n",
    "train_papers = [x[0] for x in cursor.fetchall()]\n",
    "total_papers = train_papers + test_papers\n",
    "in_params = ','.join(['%s'] * len(total_papers))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "cursor.execute(\"select b.ReferenceID, b.ReferencedByID from referencedBy b, Papers p where b.ReferencedByID in (%s) and b.ReferenceID = p.PaperID and p.`primary author` in (select AuthorID from affiliatedTo) and b.ReferenceID in (select PaperID from Papers) and b.ReferencedByID in (select PaperID from Papers)\" % in_params, total_papers)\n",
    "paper_edges = np.array(cursor.fetchall())\n",
    "all_papers = np.unique(paper_edges)\n",
    "\n",
    "remapped_papers = {all_papers[i]: i  for i in range(len(all_papers))}\n",
    "vec_remap = np.vectorize(remap, otypes=[str])\n",
    "paper_edges = vec_remap(paper_edges, remapped_papers)\n",
    "\n",
    "all_papers_list = all_papers.tolist()\n",
    "all_params = ','.join(['%s'] * len(all_papers_list))\n",
    "\n",
    "#cursor.execute(\"select p.PaperID, p.`primary author`, a.Gender, p.Title from Papers p, Authors a where p.PaperID in (%s) and p.`primary author` in (select AuthorID from affiliatedTo) and p.`primary author` = a.AuthorID\" % all_params, all_papers_list)\n",
    "cursor.execute(\"select p.PaperID, a.AuthorID, a.Gender, p.Title from Papers p, Authors a, authoredBy ab where p.PaperID in (%s) and p.PaperID = ab.PaperID and ab.AuthoredByID = a.AuthorID\" % all_params, all_papers_list) #and a.AuthorID in (select AuthorID from affiliatedTo)\n",
    "author_edges = np.array(cursor.fetchall())\n",
    "\n",
    "all_authors = np.unique(author_edges[:,1])\n",
    "remapped_authors = {all_authors[i]: i  for i in range(len(all_authors))}\n",
    "\n",
    "author_edges[:,1] = vec_remap(author_edges[:,1], remapped_authors)\n",
    "author_edges[:,0] = vec_remap(author_edges[:,0], remapped_papers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model_path = './GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def title_to_vec(title):\n",
    "    title = title.replace(\"-\", \" \")\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec = model[word.lower()]\n",
    "            vectors.append(vec)\n",
    "        except KeyError:\n",
    "            #print(word)\n",
    "            continue\n",
    "    title_vector = np.zeros(300)\n",
    "    try:\n",
    "        title_vector = sum(vectors) / len(vectors)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    return title_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9780/2618123609.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  paper_feats = torch.tensor(paper_feats)\n"
     ]
    }
   ],
   "source": [
    "all_papers_info = pd.DataFrame(author_edges)\n",
    "all_papers_info[0] = all_papers_info[0].astype(int)\n",
    "all_papers_info = all_papers_info.sort_values(0)\n",
    "paper_feats = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for title in all_papers_info[3]:\n",
    "    title_vec = title_to_vec(title)\n",
    "    paper_feats.append(title_vec)\n",
    "paper_feats = torch.tensor(paper_feats)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "headers = {\"x-api-key\": \"M7HSjQNeTfai6l7JUiDZB8XYc85BHnHt3R0NXSEd\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def request_author_information(authors):\n",
    "    resp = requests.post(\"https://api.semanticscholar.org/graph/v1/author/batch?fields=papers.title,affiliations,paperCount,citationCount,hIndex\", headers=headers, json={\"ids\": authors}).json()\n",
    "    return resp\n",
    "\n",
    "def request_author_individual_dfinformation(author):\n",
    "    resp = requests.get(f\"https://api.semanticscholar.org/graph/v1/author/{author}?fields=papers.title,paperCount,citationCount,hIndex\", headers=headers).json()\n",
    "    return resp\n",
    "\n",
    "def convert_history_to_vector(response):\n",
    "    if not response:\n",
    "        author_hist = np.zeros(300)\n",
    "        author_hist = np.append(author_hist, [0,0,0])\n",
    "        return author_hist\n",
    "\n",
    "    author_papers = response[\"papers\"]\n",
    "    author_hist = []\n",
    "\n",
    "    for paper in author_papers:\n",
    "        paperID = paper[\"paperId\"]\n",
    "        if paperID in test_papers:\n",
    "            continue\n",
    "        title = paper[\"title\"]\n",
    "        title_vect = title_to_vec(title)\n",
    "        if np.count_nonzero(title_vect) > 0:\n",
    "            author_hist.append(title_vect)\n",
    "    if len(author_hist) != 0:\n",
    "        author_hist = np.mean(np.array(author_hist), axis=0)\n",
    "    else:\n",
    "        author_hist = np.zeros(300)\n",
    "\n",
    "    author_paperCount = response[\"paperCount\"]\n",
    "    author_citationCount = response[\"citationCount\"]\n",
    "    author_hIndex = response[\"hIndex\"]\n",
    "    author_hist = np.append(author_hist, [author_paperCount, author_citationCount, author_hIndex])\n",
    "\n",
    "    return author_hist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'47399096'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors[101791]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([101791]),)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(all_authors == '47399096')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array([10001427, 10007124, 10007273, ...,  9994893,  9995883,  9998857])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "np.savetxt('all_authors.txt', all_authors.astype(int))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
