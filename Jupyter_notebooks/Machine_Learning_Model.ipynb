{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "from dgl.nn import EdgeWeightNorm, GraphConv\n",
    "from dgl.data import DGLDataset\n",
    "import mysql.connector\n",
    "import dgl.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dgl import save_graphs, load_graphs\n",
    "import networkx as nx\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Connect to database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "cnx = mysql.connector.connect(user='david', password='daviddung1993',\n",
    "                          host='127.0.0.1',\n",
    "                          database='computervision')\n",
    "cursor = cnx.cursor()\n",
    "headers = {\"x-api-key\": \"M7HSjQNeTfai6l7JUiDZB8XYc85BHnHt3R0NXSEd\"}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "2055: Cursor is not connected",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mProgrammingError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/cursor_cext.py:249\u001B[0m, in \u001B[0;36mCMySQLCursor.execute\u001B[0;34m(self, operation, params, multi)\u001B[0m\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cnx \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cnx\u001B[38;5;241m.\u001B[39mis_closed():\n\u001B[0;32m--> 249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ProgrammingError\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProgrammingError, \u001B[38;5;167;01mReferenceError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[0;31mProgrammingError\u001B[0m: Unknown error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mProgrammingError\u001B[0m                          Traceback (most recent call last)",
      "Input \u001B[0;32mIn [161]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcursor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mselect p.PaperID from Papers p, authoredBy a where p.PaperID = a.PaperID and p.Leaf = False and p.Pub_Year = 2022 and a.AuthoredByID in (select distinct(a.AuthoredByID) as authorID from Papers p, authoredBy a where p.PaperID = a.PaperID and p.Leaf = FALSE and p.Pub_Year != 2022 and a.AuthoredByID != 0) group by p.PaperID\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m test_set \u001B[38;5;241m=\u001B[39m [x[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m cursor\u001B[38;5;241m.\u001B[39mfetchall()]\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/cursor_cext.py:251\u001B[0m, in \u001B[0;36mCMySQLCursor.execute\u001B[0;34m(self, operation, params, multi)\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ProgrammingError\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProgrammingError, \u001B[38;5;167;01mReferenceError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ProgrammingError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCursor is not connected\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m2055\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cnx\u001B[38;5;241m.\u001B[39mhandle_unread_result()\n\u001B[1;32m    254\u001B[0m stmt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mProgrammingError\u001B[0m: 2055: Cursor is not connected"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"select p.PaperID from Papers p, authoredBy a where p.PaperID = a.PaperID and p.Leaf = False and p.Pub_Year = 2022 and a.AuthoredByID in (select distinct(a.AuthoredByID) as authorID from Papers p, authoredBy a where p.PaperID = a.PaperID and p.Leaf = FALSE and p.Pub_Year != 2022 and a.AuthoredByID != 0) group by p.PaperID\")\n",
    "test_set = [x[0] for x in cursor.fetchall()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DGL Dataset classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class PaperDataset(DGLDataset):\n",
    "    def __init__(self, nodes_data, edges_data):\n",
    "        super().__init__(name='paper_dataset')\n",
    "        self.nodes_data = nodes_data\n",
    "        self.edges_data = edges_data\n",
    "\n",
    "    def process(self):\n",
    "        node_features = torch.from_numpy(self.nodes_data['Year'].to_numpy())\n",
    "        node_labels = torch.from_numpy(self.nodes_data['PaperID'].astype('category').cat.codes.to_numpy())\n",
    "        edge_features = torch.from_numpy(self.edges_data['Weight'].to_numpy())\n",
    "        edges_src = torch.from_numpy(self.edges_data['Src'].to_numpy())\n",
    "        edges_dst = torch.from_numpy(self.edges_data['Dst'].to_numpy())\n",
    "\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=self.nodes_data.shape[0])\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        self.graph.edata['weight'] = edge_features\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "        n_nodes = self.nodes_data.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class AuthorDataset(DGLDataset):\n",
    "    def __init__(self, nodes_data, edges_data):\n",
    "        self.nodes_data = nodes_data\n",
    "        self.edges_data = edges_data\n",
    "        super().__init__(name='author_dataset')\n",
    "\n",
    "    def process(self):\n",
    "        #node_features = torch.from_numpy(self.nodes_data['Year'].to_numpy())\n",
    "        node_features = torch.from_numpy(np.zeros((len(self.nodes_data), 1)))\n",
    "        node_labels = torch.from_numpy(self.nodes_data['AuthorID'].astype('category').cat.codes.to_numpy())\n",
    "        edge_features = torch.from_numpy(self.edges_data['Count'].to_numpy()).type(torch.int32)\n",
    "        edges_src = torch.from_numpy(self.edges_data['RefAuthor'].astype(int).to_numpy())\n",
    "        edges_dst = torch.from_numpy(self.edges_data['Author'].astype(int).to_numpy())\n",
    "\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=self.nodes_data.shape[0])\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        self.graph.edata['weight'] = edge_features\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "        n_nodes = self.nodes_data.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "cnx.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paper Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "graph_size = 50000\n",
    "cursor.execute(\"select r.ReferenceID as rID, p.PaperID as pID from Papers p, referencedBy r where p.Leaf = False and p.Pub_Year < 2010 and r.ReferencedByID = p.PaperID and r.ReferenceID not like '11111%'\")\n",
    "\n",
    "# Fetch references (edges)\n",
    "references_raw = cursor.fetchall()\n",
    "references_df = pd.DataFrame(references_raw, columns=[\"ReferenceID\",\"PaperID\"])\n",
    "\n",
    "# Fetch all papers (nodes)\n",
    "cursor.execute(\"CREATE TEMPORARY TABLE temp_table AS (select p.PaperID as PaperID from Papers p, referencedBy r where p.Leaf = False and p.Pub_Year < 2013 and r.ReferencedByID = p.PaperID and r.ReferenceID not like '11111%' UNION select r.ReferenceID as PaperID from Papers p, referencedBy r where p.Leaf = False and p.Pub_Year < 2010 and r.ReferencedByID = p.PaperID and r.ReferenceID not like '11111%')\")\n",
    "cursor.execute(\"select * from temp_table\")\n",
    "all_papers_series = cursor.fetchall()\n",
    "all_papers_remap_ids = {all_papers_series[x][0]: x for x in range(0, len(all_papers_series))}\n",
    "\n",
    "# Remap the references to id\n",
    "references_df = references_df.applymap(lambda x: all_papers_remap_ids[x])\n",
    "\n",
    "# Fetch authors and remap to label\n",
    "cursor.execute(\"select b.PaperID, b.AuthoredByID from authoredBy b, temp_table t where b.PaperID = t.PaperID\")\n",
    "authored_by_df = pd.DataFrame(cursor.fetchall(), columns=[\"PaperID\", \"AuthorID\"])\n",
    "feature_vector_df = pd.DataFrame(columns=authored_by_df[\"AuthorID\"].unique())\n",
    "authored_by_df[\"PaperID\"] = authored_by_df[\"PaperID\"].apply(lambda x: all_papers_remap_ids[x])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4100\n",
      "4110\n",
      "4120\n",
      "4130\n",
      "4140\n",
      "4150\n",
      "4160\n",
      "4170\n",
      "4180\n",
      "4190\n",
      "4200\n",
      "4210\n",
      "4220\n",
      "4230\n",
      "4240\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n",
      "4900\n",
      "4910\n",
      "4920\n",
      "4930\n",
      "4940\n",
      "4950\n",
      "4960\n",
      "4970\n",
      "4980\n",
      "4990\n",
      "5000\n",
      "5010\n",
      "5020\n",
      "5030\n",
      "5040\n",
      "5050\n",
      "5060\n",
      "5070\n",
      "5080\n",
      "5090\n",
      "5100\n",
      "5110\n",
      "5120\n",
      "5130\n",
      "5140\n",
      "5150\n",
      "5160\n",
      "5170\n",
      "5180\n",
      "5190\n",
      "5200\n",
      "5210\n",
      "5220\n",
      "5230\n",
      "5240\n",
      "5250\n",
      "5260\n",
      "5270\n",
      "5280\n",
      "5290\n",
      "5300\n",
      "5310\n",
      "5320\n",
      "5330\n",
      "5340\n",
      "5350\n",
      "5360\n",
      "5370\n",
      "5380\n",
      "5390\n",
      "5400\n",
      "5410\n",
      "5420\n",
      "5430\n",
      "5440\n",
      "5450\n",
      "5460\n",
      "5470\n",
      "5480\n",
      "5490\n",
      "5500\n",
      "5510\n",
      "5520\n",
      "5530\n",
      "5540\n",
      "5550\n",
      "5560\n",
      "5570\n",
      "5580\n",
      "5590\n",
      "5600\n",
      "5610\n",
      "5620\n",
      "5630\n",
      "5640\n",
      "5650\n",
      "5660\n",
      "5670\n",
      "5680\n",
      "5690\n",
      "5700\n",
      "5710\n",
      "5720\n",
      "5730\n",
      "5740\n",
      "5750\n",
      "5760\n",
      "5770\n",
      "5780\n",
      "5790\n",
      "5800\n",
      "5810\n",
      "5820\n",
      "5830\n",
      "5840\n",
      "5850\n",
      "5860\n",
      "5870\n",
      "5880\n",
      "5890\n",
      "5900\n",
      "5910\n",
      "5920\n",
      "5930\n",
      "5940\n",
      "5950\n",
      "5960\n",
      "5970\n",
      "5980\n",
      "5990\n",
      "6000\n",
      "6010\n",
      "6020\n",
      "6030\n",
      "6040\n",
      "6050\n",
      "6060\n",
      "6070\n",
      "6080\n",
      "6090\n",
      "6100\n",
      "6110\n",
      "6120\n",
      "6130\n",
      "6140\n",
      "6150\n",
      "6160\n",
      "6170\n",
      "6180\n",
      "6190\n",
      "6200\n",
      "6210\n",
      "6220\n",
      "6230\n",
      "6240\n",
      "6250\n",
      "6260\n",
      "6270\n",
      "6280\n",
      "6290\n",
      "6300\n",
      "6310\n",
      "6320\n",
      "6330\n",
      "6340\n",
      "6350\n",
      "6360\n",
      "6370\n",
      "6380\n",
      "6390\n",
      "6400\n",
      "6410\n",
      "6420\n",
      "6430\n",
      "6440\n",
      "6450\n",
      "6460\n",
      "6470\n",
      "6480\n",
      "6490\n",
      "6500\n",
      "6510\n",
      "6520\n",
      "6530\n",
      "6540\n",
      "6550\n",
      "6560\n",
      "6570\n",
      "6580\n",
      "6590\n",
      "6600\n",
      "6610\n",
      "6620\n",
      "6630\n",
      "6640\n",
      "6650\n",
      "6660\n",
      "6670\n",
      "6680\n",
      "6690\n",
      "6700\n",
      "6710\n",
      "6720\n",
      "6730\n",
      "6740\n",
      "6750\n",
      "6760\n",
      "6770\n",
      "6780\n",
      "6790\n",
      "6800\n",
      "6810\n",
      "6820\n",
      "6830\n",
      "6840\n",
      "6850\n",
      "6860\n",
      "6870\n",
      "6880\n",
      "6890\n",
      "6900\n",
      "6910\n",
      "6920\n",
      "6930\n",
      "6940\n",
      "6950\n",
      "6960\n",
      "6970\n",
      "6980\n",
      "6990\n",
      "7000\n",
      "7010\n",
      "7020\n",
      "7030\n",
      "7040\n",
      "7050\n",
      "7060\n",
      "7070\n",
      "7080\n",
      "7090\n",
      "7100\n",
      "7110\n",
      "7120\n",
      "7130\n",
      "7140\n",
      "7150\n",
      "7160\n",
      "7170\n",
      "7180\n",
      "7190\n",
      "7200\n",
      "7210\n",
      "7220\n",
      "7230\n",
      "7240\n",
      "7250\n",
      "7260\n",
      "7270\n",
      "7280\n",
      "7290\n",
      "7300\n",
      "7310\n",
      "7320\n",
      "7330\n",
      "7340\n",
      "7350\n",
      "7360\n",
      "7370\n",
      "7380\n",
      "7390\n",
      "7400\n",
      "7410\n",
      "7420\n",
      "7430\n",
      "7440\n",
      "7450\n",
      "7460\n",
      "7470\n",
      "7480\n",
      "7490\n",
      "7500\n",
      "7510\n",
      "7520\n",
      "7530\n",
      "7540\n",
      "7550\n",
      "7560\n",
      "7570\n",
      "7580\n",
      "7590\n",
      "7600\n",
      "7610\n",
      "7620\n",
      "7630\n",
      "7640\n",
      "7650\n",
      "7660\n",
      "7670\n",
      "7680\n",
      "7690\n",
      "7700\n",
      "7710\n",
      "7720\n",
      "7730\n",
      "7740\n",
      "7750\n",
      "7760\n",
      "7770\n",
      "7780\n",
      "7790\n",
      "7800\n",
      "7810\n",
      "7820\n",
      "7830\n",
      "7840\n",
      "7850\n",
      "7860\n",
      "7870\n",
      "7880\n",
      "7890\n",
      "7900\n",
      "7910\n",
      "7920\n",
      "7930\n",
      "7940\n",
      "7950\n",
      "7960\n",
      "7970\n",
      "7980\n",
      "7990\n",
      "8000\n",
      "8010\n",
      "8020\n",
      "8030\n",
      "8040\n",
      "8050\n",
      "8060\n",
      "8070\n",
      "8080\n",
      "8090\n",
      "8100\n",
      "8110\n",
      "8120\n",
      "8130\n",
      "8140\n",
      "8150\n",
      "8160\n",
      "8170\n",
      "8180\n",
      "8190\n",
      "8200\n",
      "8210\n",
      "8220\n",
      "8230\n",
      "8240\n",
      "8250\n",
      "8260\n",
      "8270\n",
      "8280\n",
      "8290\n",
      "8300\n",
      "8310\n",
      "8320\n",
      "8330\n",
      "8340\n",
      "8350\n",
      "8360\n",
      "8370\n",
      "8380\n",
      "8390\n",
      "8400\n",
      "8410\n",
      "8420\n",
      "8430\n",
      "8440\n",
      "8450\n",
      "8460\n",
      "8470\n",
      "8480\n",
      "8490\n",
      "8500\n",
      "8510\n",
      "8520\n",
      "8530\n",
      "8540\n",
      "8550\n",
      "8560\n",
      "8570\n",
      "8580\n",
      "8590\n",
      "8600\n",
      "8610\n",
      "8620\n",
      "8630\n",
      "8640\n",
      "8650\n",
      "8660\n",
      "8670\n",
      "8680\n",
      "8690\n",
      "8700\n",
      "8710\n",
      "8720\n",
      "8730\n",
      "8740\n",
      "8750\n",
      "8760\n",
      "8770\n",
      "8780\n",
      "8790\n",
      "8800\n",
      "8810\n",
      "8820\n",
      "8830\n",
      "8840\n",
      "8850\n",
      "8860\n",
      "8870\n",
      "8880\n",
      "8890\n",
      "8900\n",
      "8910\n",
      "8920\n",
      "8930\n",
      "8940\n",
      "8950\n",
      "8960\n",
      "8970\n",
      "8980\n",
      "8990\n",
      "9000\n",
      "9010\n",
      "9020\n",
      "9030\n",
      "9040\n",
      "9050\n",
      "9060\n",
      "9070\n",
      "9080\n",
      "9090\n",
      "9100\n",
      "9110\n",
      "9120\n",
      "9130\n",
      "9140\n",
      "9150\n",
      "9160\n",
      "9170\n",
      "9180\n",
      "9190\n",
      "9200\n",
      "9210\n",
      "9220\n",
      "9230\n",
      "9240\n",
      "9250\n",
      "9260\n",
      "9270\n",
      "9280\n",
      "9290\n",
      "9300\n",
      "9310\n",
      "9320\n",
      "9330\n",
      "9340\n",
      "9350\n",
      "9360\n",
      "9370\n",
      "9380\n",
      "9390\n",
      "9400\n",
      "9410\n",
      "9420\n",
      "9430\n",
      "9440\n",
      "9450\n",
      "9460\n",
      "9470\n",
      "9480\n",
      "9490\n",
      "9500\n",
      "9510\n",
      "9520\n",
      "9530\n",
      "9540\n",
      "9550\n",
      "9560\n",
      "9570\n",
      "9580\n",
      "9590\n",
      "9600\n",
      "9610\n",
      "9620\n",
      "9630\n",
      "9640\n",
      "9650\n",
      "9660\n",
      "9670\n",
      "9680\n",
      "9690\n",
      "9700\n",
      "9710\n",
      "9720\n",
      "9730\n",
      "9740\n",
      "9750\n",
      "9760\n",
      "9770\n",
      "9780\n",
      "9790\n",
      "9800\n",
      "9810\n",
      "9820\n",
      "9830\n",
      "9840\n",
      "9850\n",
      "9860\n",
      "9870\n",
      "9880\n",
      "9890\n",
      "9900\n",
      "9910\n",
      "9920\n",
      "9930\n",
      "9940\n",
      "9950\n",
      "9960\n",
      "9970\n",
      "9980\n",
      "9990\n",
      "10000\n",
      "10010\n",
      "10020\n",
      "10030\n",
      "10040\n",
      "10050\n",
      "10060\n",
      "10070\n",
      "10080\n",
      "10090\n",
      "10100\n",
      "10110\n",
      "10120\n",
      "10130\n",
      "10140\n",
      "10150\n",
      "10160\n",
      "10170\n",
      "10180\n",
      "10190\n",
      "10200\n",
      "10210\n",
      "10220\n",
      "10230\n",
      "10240\n",
      "10250\n",
      "10260\n",
      "10270\n",
      "10280\n",
      "10290\n",
      "10300\n",
      "10310\n",
      "10320\n",
      "10330\n",
      "10340\n",
      "10350\n",
      "10360\n",
      "10370\n",
      "10380\n",
      "10390\n",
      "10400\n",
      "10410\n",
      "10420\n",
      "10430\n",
      "10440\n",
      "10450\n",
      "10460\n",
      "10470\n",
      "10480\n",
      "10490\n",
      "10500\n",
      "10510\n",
      "10520\n",
      "10530\n",
      "10540\n",
      "10550\n",
      "10560\n",
      "10570\n",
      "10580\n",
      "10590\n",
      "10600\n",
      "10610\n",
      "10620\n",
      "10630\n",
      "10640\n",
      "10650\n",
      "10660\n",
      "10670\n",
      "10680\n",
      "10690\n",
      "10700\n",
      "10710\n",
      "10720\n",
      "10730\n",
      "10740\n",
      "10750\n",
      "10760\n",
      "10770\n",
      "10780\n",
      "10790\n",
      "10800\n",
      "10810\n",
      "10820\n",
      "10830\n",
      "10840\n",
      "10850\n",
      "10860\n",
      "10870\n",
      "10880\n",
      "10890\n",
      "10900\n",
      "10910\n",
      "10920\n",
      "10930\n",
      "10940\n",
      "10950\n",
      "10960\n",
      "10970\n",
      "10980\n",
      "10990\n",
      "11000\n",
      "11010\n",
      "11020\n",
      "11030\n",
      "11040\n",
      "11050\n",
      "11060\n",
      "11070\n",
      "11080\n",
      "11090\n",
      "11100\n",
      "11110\n",
      "11120\n",
      "11130\n",
      "11140\n",
      "11150\n",
      "11160\n",
      "11170\n",
      "11180\n",
      "11190\n",
      "11200\n",
      "11210\n",
      "11220\n",
      "11230\n",
      "11240\n",
      "11250\n",
      "11260\n",
      "11270\n",
      "11280\n",
      "11290\n",
      "11300\n",
      "11310\n",
      "11320\n",
      "11330\n",
      "11340\n",
      "11350\n",
      "11360\n",
      "11370\n",
      "11380\n",
      "11390\n",
      "11400\n",
      "11410\n",
      "11420\n",
      "11430\n",
      "11440\n",
      "11450\n",
      "11460\n",
      "11470\n",
      "11480\n",
      "11490\n",
      "11500\n",
      "11510\n",
      "11520\n",
      "11530\n",
      "11540\n",
      "11550\n",
      "11560\n",
      "11570\n",
      "11580\n",
      "11590\n",
      "11600\n",
      "11610\n",
      "11620\n",
      "11630\n",
      "11640\n",
      "11650\n",
      "11660\n",
      "11670\n",
      "11680\n",
      "11690\n",
      "11700\n",
      "11710\n",
      "11720\n",
      "11730\n",
      "11740\n",
      "11750\n",
      "11760\n",
      "11770\n",
      "11780\n",
      "11790\n",
      "11800\n",
      "11810\n",
      "11820\n",
      "11830\n",
      "11840\n",
      "11850\n",
      "11860\n",
      "11870\n",
      "11880\n",
      "11890\n",
      "11900\n",
      "11910\n",
      "11920\n",
      "11930\n",
      "11940\n",
      "11950\n",
      "11960\n",
      "11970\n",
      "11980\n",
      "11990\n",
      "12000\n",
      "12010\n",
      "12020\n",
      "12030\n",
      "12040\n",
      "12050\n",
      "12060\n",
      "12070\n",
      "12080\n",
      "12090\n",
      "12100\n",
      "12110\n",
      "12120\n",
      "12130\n",
      "12140\n",
      "12150\n",
      "12160\n",
      "12170\n",
      "12180\n",
      "12190\n",
      "12200\n",
      "12210\n",
      "12220\n",
      "12230\n",
      "12240\n",
      "12250\n",
      "12260\n",
      "12270\n",
      "12280\n",
      "12290\n",
      "12300\n",
      "12310\n",
      "12320\n",
      "12330\n",
      "12340\n",
      "12350\n",
      "12360\n",
      "12370\n",
      "12380\n",
      "12390\n",
      "12400\n",
      "12410\n",
      "12420\n",
      "12430\n",
      "12440\n",
      "12450\n",
      "12460\n",
      "12470\n",
      "12480\n",
      "12490\n",
      "12500\n",
      "12510\n",
      "12520\n",
      "12530\n",
      "12540\n",
      "12550\n",
      "12560\n",
      "12570\n",
      "12580\n",
      "12590\n",
      "12600\n",
      "12610\n",
      "12620\n",
      "12630\n",
      "12640\n",
      "12650\n",
      "12660\n",
      "12670\n",
      "12680\n",
      "12690\n",
      "12700\n",
      "12710\n",
      "12720\n",
      "12730\n",
      "12740\n",
      "12750\n",
      "12760\n",
      "12770\n",
      "12780\n",
      "12790\n",
      "12800\n",
      "12810\n",
      "12820\n",
      "12830\n",
      "12840\n",
      "12850\n",
      "12860\n",
      "12870\n",
      "12880\n",
      "12890\n",
      "12900\n",
      "12910\n",
      "12920\n",
      "12930\n",
      "12940\n",
      "12950\n",
      "12960\n",
      "12970\n",
      "12980\n",
      "12990\n",
      "13000\n",
      "13010\n",
      "13020\n",
      "13030\n",
      "13040\n",
      "13050\n",
      "13060\n",
      "13070\n",
      "13080\n",
      "13090\n",
      "13100\n",
      "13110\n",
      "13120\n",
      "13130\n",
      "13140\n",
      "13150\n",
      "13160\n",
      "13170\n",
      "13180\n",
      "13190\n",
      "13200\n",
      "13210\n",
      "13220\n",
      "13230\n",
      "13240\n",
      "13250\n",
      "13260\n",
      "13270\n",
      "13280\n",
      "13290\n",
      "13300\n",
      "13310\n",
      "13320\n",
      "13330\n",
      "13340\n",
      "13350\n",
      "13360\n",
      "13370\n",
      "13380\n",
      "13390\n",
      "13400\n",
      "13410\n",
      "13420\n",
      "13430\n",
      "13440\n",
      "13450\n",
      "13460\n",
      "13470\n",
      "13480\n",
      "13490\n",
      "13500\n",
      "13510\n",
      "13520\n",
      "13530\n",
      "13540\n",
      "13550\n",
      "13560\n",
      "13570\n",
      "13580\n",
      "13590\n",
      "13600\n",
      "13610\n",
      "13620\n",
      "13630\n",
      "13640\n",
      "13650\n",
      "13660\n",
      "13670\n",
      "13680\n",
      "13690\n",
      "13700\n",
      "13710\n",
      "13720\n",
      "13730\n",
      "13740\n",
      "13750\n",
      "13760\n",
      "13770\n",
      "13780\n",
      "13790\n",
      "13800\n",
      "13810\n",
      "13820\n",
      "13830\n",
      "13840\n",
      "13850\n",
      "13860\n",
      "13870\n",
      "13880\n",
      "13890\n",
      "13900\n",
      "13910\n",
      "13920\n",
      "13930\n",
      "13940\n",
      "13950\n",
      "13960\n",
      "13970\n",
      "13980\n",
      "13990\n",
      "14000\n",
      "14010\n",
      "14020\n",
      "14030\n",
      "14040\n",
      "14050\n",
      "14060\n",
      "14070\n",
      "14080\n",
      "14090\n",
      "14100\n",
      "14110\n",
      "14120\n",
      "14130\n",
      "14140\n",
      "14150\n",
      "14160\n",
      "14170\n",
      "14180\n",
      "14190\n",
      "14200\n",
      "14210\n",
      "14220\n",
      "14230\n",
      "14240\n",
      "14250\n",
      "14260\n",
      "14270\n",
      "14280\n",
      "14290\n",
      "14300\n",
      "14310\n",
      "14320\n",
      "14330\n",
      "14340\n",
      "14350\n",
      "14360\n",
      "14370\n",
      "14380\n",
      "14390\n",
      "14400\n",
      "14410\n",
      "14420\n",
      "14430\n",
      "14440\n",
      "14450\n",
      "14460\n",
      "14470\n",
      "14480\n",
      "14490\n",
      "14500\n",
      "14510\n",
      "14520\n",
      "14530\n",
      "14540\n",
      "14550\n",
      "14560\n",
      "14570\n",
      "14580\n",
      "14590\n",
      "14600\n",
      "14610\n",
      "14620\n",
      "14630\n",
      "14640\n",
      "14650\n",
      "14660\n",
      "14670\n",
      "14680\n",
      "14690\n",
      "14700\n",
      "14710\n",
      "14720\n",
      "14730\n",
      "14740\n",
      "14750\n",
      "14760\n",
      "14770\n",
      "14780\n",
      "14790\n",
      "14800\n",
      "14810\n",
      "14820\n",
      "14830\n",
      "14840\n",
      "14850\n",
      "14860\n",
      "14870\n",
      "14880\n",
      "14890\n",
      "14900\n",
      "14910\n",
      "14920\n",
      "14930\n",
      "14940\n",
      "14950\n",
      "14960\n",
      "14970\n",
      "14980\n",
      "14990\n",
      "15000\n",
      "15010\n",
      "15020\n",
      "15030\n",
      "15040\n",
      "15050\n",
      "15060\n",
      "15070\n",
      "15080\n",
      "15090\n",
      "15100\n",
      "15110\n",
      "15120\n",
      "15130\n",
      "15140\n",
      "15150\n",
      "15160\n",
      "15170\n",
      "15180\n",
      "15190\n",
      "15200\n",
      "15210\n",
      "15220\n",
      "15230\n",
      "15240\n",
      "15250\n",
      "15260\n",
      "15270\n",
      "15280\n",
      "15290\n",
      "15300\n",
      "15310\n",
      "15320\n",
      "15330\n",
      "15340\n",
      "15350\n",
      "15360\n",
      "15370\n",
      "15380\n",
      "15390\n",
      "15400\n",
      "15410\n",
      "15420\n",
      "15430\n",
      "15440\n",
      "15450\n",
      "15460\n",
      "15470\n",
      "15480\n",
      "15490\n",
      "15500\n",
      "15510\n",
      "15520\n",
      "15530\n",
      "15540\n",
      "15550\n",
      "15560\n",
      "15570\n",
      "15580\n",
      "15590\n",
      "15600\n",
      "15610\n",
      "15620\n",
      "15630\n",
      "15640\n",
      "15650\n",
      "15660\n",
      "15670\n",
      "15680\n",
      "15690\n",
      "15700\n",
      "15710\n",
      "15720\n",
      "15730\n",
      "15740\n",
      "15750\n",
      "15760\n",
      "15770\n",
      "15780\n",
      "15790\n",
      "15800\n",
      "15810\n",
      "15820\n",
      "15830\n",
      "15840\n",
      "15850\n",
      "15860\n",
      "15870\n",
      "15880\n",
      "15890\n",
      "15900\n",
      "15910\n",
      "15920\n",
      "15930\n",
      "15940\n",
      "15950\n",
      "15960\n",
      "15970\n",
      "15980\n",
      "15990\n",
      "16000\n",
      "16010\n",
      "16020\n",
      "16030\n",
      "16040\n",
      "16050\n",
      "16060\n",
      "16070\n",
      "16080\n",
      "16090\n",
      "16100\n",
      "16110\n",
      "16120\n",
      "16130\n",
      "16140\n",
      "16150\n",
      "16160\n",
      "16170\n",
      "16180\n",
      "16190\n",
      "16200\n",
      "16210\n",
      "16220\n",
      "16230\n",
      "16240\n",
      "16250\n",
      "16260\n",
      "16270\n",
      "16280\n",
      "16290\n",
      "16300\n",
      "16310\n",
      "16320\n",
      "16330\n",
      "16340\n",
      "16350\n",
      "16360\n",
      "16370\n",
      "16380\n",
      "16390\n",
      "16400\n",
      "16410\n",
      "16420\n",
      "16430\n",
      "16440\n",
      "16450\n",
      "16460\n",
      "16470\n",
      "16480\n",
      "16490\n",
      "16500\n",
      "16510\n",
      "16520\n",
      "16530\n",
      "16540\n",
      "16550\n",
      "16560\n",
      "16570\n",
      "16580\n",
      "16590\n",
      "16600\n",
      "16610\n",
      "16620\n",
      "16630\n",
      "16640\n",
      "16650\n",
      "16660\n",
      "16670\n",
      "16680\n",
      "16690\n",
      "16700\n",
      "16710\n",
      "16720\n",
      "16730\n",
      "16740\n",
      "16750\n",
      "16760\n",
      "16770\n",
      "16780\n",
      "16790\n",
      "16800\n",
      "16810\n",
      "16820\n",
      "16830\n",
      "16840\n",
      "16850\n",
      "16860\n",
      "16870\n",
      "16880\n",
      "16890\n",
      "16900\n",
      "16910\n",
      "16920\n",
      "16930\n",
      "16940\n",
      "16950\n",
      "16960\n",
      "16970\n",
      "16980\n",
      "16990\n",
      "17000\n",
      "17010\n",
      "17020\n",
      "17030\n",
      "17040\n",
      "17050\n",
      "17060\n",
      "17070\n",
      "17080\n",
      "17090\n",
      "17100\n",
      "17110\n",
      "17120\n",
      "17130\n",
      "17140\n",
      "17150\n",
      "17160\n",
      "17170\n",
      "17180\n",
      "17190\n",
      "17200\n",
      "17210\n",
      "17220\n",
      "17230\n",
      "17240\n",
      "17250\n",
      "17260\n",
      "17270\n",
      "17280\n",
      "17290\n",
      "17300\n",
      "17310\n",
      "17320\n",
      "17330\n",
      "17340\n",
      "17350\n",
      "17360\n",
      "17370\n",
      "17380\n",
      "17390\n",
      "17400\n",
      "17410\n",
      "17420\n",
      "17430\n",
      "17440\n",
      "17450\n",
      "17460\n",
      "17470\n",
      "17480\n",
      "17490\n",
      "17500\n",
      "17510\n",
      "17520\n",
      "17530\n",
      "17540\n",
      "17550\n",
      "17560\n",
      "17570\n",
      "17580\n",
      "17590\n",
      "17600\n",
      "17610\n",
      "17620\n",
      "17630\n",
      "17640\n",
      "17650\n",
      "17660\n",
      "17670\n",
      "17680\n",
      "17690\n",
      "17700\n",
      "17710\n",
      "17720\n",
      "17730\n",
      "17740\n",
      "17750\n",
      "17760\n",
      "17770\n",
      "17780\n",
      "17790\n",
      "17800\n",
      "17810\n",
      "17820\n",
      "17830\n",
      "17840\n",
      "17850\n",
      "17860\n",
      "17870\n",
      "17880\n",
      "17890\n",
      "17900\n",
      "17910\n",
      "17920\n",
      "17930\n",
      "17940\n",
      "17950\n",
      "17960\n",
      "17970\n",
      "17980\n",
      "17990\n",
      "18000\n",
      "18010\n",
      "18020\n",
      "18030\n",
      "18040\n",
      "18050\n",
      "18060\n",
      "18070\n",
      "18080\n",
      "18090\n",
      "18100\n",
      "18110\n",
      "18120\n",
      "18130\n",
      "18140\n",
      "18150\n",
      "18160\n",
      "18170\n",
      "18180\n",
      "18190\n",
      "18200\n",
      "18210\n",
      "18220\n",
      "18230\n",
      "18240\n",
      "18250\n",
      "18260\n",
      "18270\n",
      "18280\n",
      "18290\n",
      "18300\n",
      "18310\n",
      "18320\n",
      "18330\n",
      "18340\n",
      "18350\n",
      "18360\n",
      "18370\n",
      "18380\n",
      "18390\n",
      "18400\n",
      "18410\n",
      "18420\n",
      "18430\n",
      "18440\n",
      "18450\n",
      "18460\n",
      "18470\n",
      "18480\n",
      "18490\n",
      "18500\n",
      "18510\n",
      "18520\n",
      "18530\n",
      "18540\n",
      "18550\n",
      "18560\n",
      "18570\n",
      "18580\n",
      "18590\n",
      "18600\n",
      "18610\n",
      "18620\n",
      "18630\n",
      "18640\n",
      "18650\n",
      "18660\n",
      "18670\n",
      "18680\n",
      "18690\n",
      "18700\n",
      "18710\n",
      "18720\n",
      "18730\n",
      "18740\n",
      "18750\n",
      "18760\n",
      "18770\n",
      "18780\n",
      "18790\n",
      "18800\n",
      "18810\n",
      "18820\n",
      "18830\n",
      "18840\n",
      "18850\n",
      "18860\n",
      "18870\n",
      "18880\n",
      "18890\n",
      "18900\n",
      "18910\n",
      "18920\n",
      "18930\n",
      "18940\n",
      "18950\n",
      "18960\n",
      "18970\n",
      "18980\n",
      "18990\n",
      "19000\n",
      "19010\n",
      "19020\n",
      "19030\n",
      "19040\n",
      "19050\n",
      "19060\n",
      "19070\n",
      "19080\n",
      "19090\n",
      "19100\n",
      "19110\n",
      "19120\n",
      "19130\n",
      "19140\n",
      "19150\n",
      "19160\n",
      "19170\n",
      "19180\n",
      "19190\n",
      "19200\n",
      "19210\n",
      "19220\n",
      "19230\n",
      "19240\n",
      "19250\n",
      "19260\n",
      "19270\n",
      "19280\n",
      "19290\n",
      "19300\n",
      "19310\n",
      "19320\n",
      "19330\n",
      "19340\n",
      "19350\n",
      "19360\n",
      "19370\n",
      "19380\n",
      "19390\n",
      "19400\n",
      "19410\n",
      "19420\n",
      "19430\n",
      "19440\n",
      "19450\n",
      "19460\n",
      "19470\n",
      "19480\n",
      "19490\n",
      "19500\n",
      "19510\n",
      "19520\n",
      "19530\n",
      "19540\n",
      "19550\n",
      "19560\n",
      "19570\n",
      "19580\n",
      "19590\n",
      "19600\n",
      "19610\n",
      "19620\n",
      "19630\n",
      "19640\n",
      "19650\n",
      "19660\n",
      "19670\n",
      "19680\n",
      "19690\n",
      "19700\n",
      "19710\n",
      "19720\n",
      "19730\n",
      "19740\n",
      "19750\n",
      "19760\n",
      "19770\n",
      "19780\n",
      "19790\n",
      "19800\n",
      "19810\n",
      "19820\n",
      "19830\n",
      "19840\n",
      "19850\n",
      "19860\n",
      "19870\n",
      "19880\n",
      "19890\n",
      "19900\n",
      "19910\n",
      "19920\n",
      "19930\n",
      "19940\n",
      "19950\n",
      "19960\n",
      "19970\n",
      "19980\n",
      "19990\n",
      "20000\n",
      "20010\n",
      "20020\n",
      "20030\n",
      "20040\n",
      "20050\n",
      "20060\n",
      "20070\n",
      "20080\n",
      "20090\n",
      "20100\n",
      "20110\n",
      "20120\n",
      "20130\n",
      "20140\n",
      "20150\n",
      "20160\n",
      "20170\n",
      "20180\n",
      "20190\n",
      "20200\n",
      "20210\n",
      "20220\n",
      "20230\n",
      "20240\n",
      "20250\n",
      "20260\n",
      "20270\n",
      "20280\n",
      "20290\n",
      "20300\n",
      "20310\n",
      "20320\n",
      "20330\n",
      "20340\n",
      "20350\n",
      "20360\n",
      "20370\n",
      "20380\n",
      "20390\n",
      "20400\n",
      "20410\n",
      "20420\n",
      "20430\n",
      "20440\n",
      "20450\n",
      "20460\n",
      "20470\n",
      "20480\n",
      "20490\n",
      "20500\n",
      "20510\n",
      "20520\n",
      "20530\n",
      "20540\n",
      "20550\n",
      "20560\n",
      "20570\n",
      "20580\n",
      "20590\n",
      "20600\n",
      "20610\n",
      "20620\n",
      "20630\n",
      "20640\n",
      "20650\n",
      "20660\n",
      "20670\n",
      "20680\n",
      "20690\n",
      "20700\n",
      "20710\n",
      "20720\n",
      "20730\n",
      "20740\n",
      "20750\n",
      "20760\n",
      "20770\n",
      "20780\n",
      "20790\n",
      "20800\n",
      "20810\n",
      "20820\n",
      "20830\n",
      "20840\n",
      "20850\n",
      "20860\n",
      "20870\n",
      "20880\n",
      "20890\n",
      "20900\n",
      "20910\n",
      "20920\n",
      "20930\n",
      "20940\n",
      "20950\n",
      "20960\n",
      "20970\n",
      "20980\n",
      "20990\n",
      "21000\n",
      "21010\n",
      "21020\n",
      "21030\n",
      "21040\n",
      "21050\n",
      "21060\n",
      "21070\n",
      "21080\n",
      "21090\n",
      "21100\n",
      "21110\n",
      "21120\n",
      "21130\n",
      "21140\n",
      "21150\n",
      "21160\n",
      "21170\n",
      "21180\n",
      "21190\n",
      "21200\n",
      "21210\n",
      "21220\n",
      "21230\n",
      "21240\n",
      "21250\n",
      "21260\n",
      "21270\n",
      "21280\n",
      "21290\n",
      "21300\n",
      "21310\n",
      "21320\n",
      "21330\n",
      "21340\n",
      "21350\n",
      "21360\n",
      "21370\n",
      "21380\n",
      "21390\n",
      "21400\n",
      "21410\n",
      "21420\n",
      "21430\n",
      "21440\n",
      "21450\n",
      "21460\n",
      "21470\n",
      "21480\n",
      "21490\n",
      "21500\n",
      "21510\n",
      "21520\n",
      "21530\n",
      "21540\n",
      "21550\n",
      "21560\n",
      "21570\n",
      "21580\n",
      "21590\n",
      "21600\n",
      "21610\n",
      "21620\n",
      "21630\n",
      "21640\n",
      "21650\n",
      "21660\n",
      "21670\n",
      "21680\n",
      "21690\n",
      "21700\n",
      "21710\n",
      "21720\n",
      "21730\n",
      "21740\n",
      "21750\n",
      "21760\n",
      "21770\n",
      "21780\n",
      "21790\n",
      "21800\n",
      "21810\n",
      "21820\n",
      "21830\n",
      "21840\n",
      "21850\n",
      "21860\n",
      "21870\n",
      "21880\n",
      "21890\n",
      "21900\n",
      "21910\n",
      "21920\n",
      "21930\n",
      "21940\n",
      "21950\n",
      "21960\n",
      "21970\n",
      "21980\n",
      "21990\n",
      "22000\n",
      "22010\n",
      "22020\n",
      "22030\n",
      "22040\n",
      "22050\n",
      "22060\n",
      "22070\n",
      "22080\n",
      "22090\n",
      "22100\n",
      "22110\n",
      "22120\n",
      "22130\n",
      "22140\n",
      "22150\n",
      "22160\n",
      "22170\n",
      "22180\n",
      "22190\n",
      "22200\n",
      "22210\n",
      "22220\n",
      "22230\n",
      "22240\n",
      "22250\n",
      "22260\n",
      "22270\n",
      "22280\n",
      "22290\n",
      "22300\n",
      "22310\n",
      "22320\n",
      "22330\n",
      "22340\n",
      "22350\n",
      "22360\n",
      "22370\n",
      "22380\n",
      "22390\n",
      "22400\n",
      "22410\n",
      "22420\n",
      "22430\n",
      "22440\n",
      "22450\n",
      "22460\n",
      "22470\n",
      "22480\n",
      "22490\n",
      "22500\n",
      "22510\n",
      "22520\n",
      "22530\n",
      "22540\n",
      "22550\n",
      "22560\n",
      "22570\n",
      "22580\n",
      "22590\n",
      "22600\n",
      "22610\n",
      "22620\n",
      "22630\n",
      "22640\n",
      "22650\n",
      "22660\n",
      "22670\n",
      "22680\n",
      "22690\n",
      "22700\n",
      "22710\n",
      "22720\n",
      "22730\n",
      "22740\n",
      "22750\n",
      "22760\n",
      "22770\n",
      "22780\n",
      "22790\n",
      "22800\n",
      "22810\n",
      "22820\n",
      "22830\n",
      "22840\n",
      "22850\n",
      "22860\n",
      "22870\n",
      "22880\n",
      "22890\n",
      "22900\n",
      "22910\n",
      "22920\n",
      "22930\n",
      "22940\n",
      "22950\n",
      "22960\n",
      "22970\n",
      "22980\n",
      "22990\n",
      "23000\n",
      "23010\n",
      "23020\n",
      "23030\n",
      "23040\n",
      "23050\n",
      "23060\n",
      "23070\n",
      "23080\n",
      "23090\n",
      "23100\n",
      "23110\n",
      "23120\n",
      "23130\n",
      "23140\n",
      "23150\n",
      "23160\n",
      "23170\n",
      "23180\n",
      "23190\n",
      "23200\n",
      "23210\n",
      "23220\n",
      "23230\n",
      "23240\n",
      "23250\n",
      "23260\n",
      "23270\n",
      "23280\n",
      "23290\n",
      "23300\n",
      "23310\n",
      "23320\n",
      "23330\n",
      "23340\n",
      "23350\n",
      "23360\n",
      "23370\n",
      "23380\n",
      "23390\n",
      "23400\n",
      "23410\n",
      "23420\n",
      "23430\n",
      "23440\n",
      "23450\n",
      "23460\n",
      "23470\n",
      "23480\n",
      "23490\n",
      "23500\n",
      "23510\n",
      "23520\n",
      "23530\n",
      "23540\n",
      "23550\n",
      "23560\n",
      "23570\n",
      "23580\n",
      "23590\n",
      "23600\n",
      "23610\n",
      "23620\n",
      "23630\n",
      "23640\n",
      "23650\n",
      "23660\n",
      "23670\n",
      "23680\n",
      "23690\n",
      "23700\n",
      "23710\n",
      "23720\n",
      "23730\n",
      "23740\n",
      "23750\n",
      "23760\n",
      "23770\n",
      "23780\n",
      "23790\n",
      "23800\n",
      "23810\n",
      "23820\n",
      "23830\n",
      "23840\n",
      "23850\n",
      "23860\n",
      "23870\n",
      "23880\n",
      "23890\n",
      "23900\n",
      "23910\n",
      "23920\n",
      "23930\n",
      "23940\n",
      "23950\n",
      "23960\n",
      "23970\n",
      "23980\n",
      "23990\n",
      "24000\n",
      "24010\n",
      "24020\n",
      "24030\n",
      "24040\n",
      "24050\n",
      "24060\n",
      "24070\n",
      "24080\n",
      "24090\n",
      "24100\n",
      "24110\n",
      "24120\n",
      "24130\n",
      "24140\n",
      "24150\n",
      "24160\n",
      "24170\n",
      "24180\n",
      "24190\n",
      "24200\n",
      "24210\n",
      "24220\n",
      "24230\n",
      "24240\n",
      "24250\n",
      "24260\n",
      "24270\n",
      "24280\n",
      "24290\n",
      "24300\n",
      "24310\n",
      "24320\n",
      "24330\n",
      "24340\n",
      "24350\n",
      "24360\n",
      "24370\n",
      "24380\n",
      "24390\n",
      "24400\n",
      "24410\n",
      "24420\n",
      "24430\n",
      "24440\n",
      "24450\n",
      "24460\n",
      "24470\n",
      "24480\n",
      "24490\n",
      "24500\n",
      "24510\n",
      "24520\n",
      "24530\n",
      "24540\n",
      "24550\n",
      "24560\n",
      "24570\n",
      "24580\n",
      "24590\n",
      "24600\n",
      "24610\n",
      "24620\n",
      "24630\n",
      "24640\n",
      "24650\n",
      "24660\n",
      "24670\n",
      "24680\n",
      "24690\n",
      "24700\n",
      "24710\n",
      "24720\n",
      "24730\n",
      "24740\n",
      "24750\n",
      "24760\n",
      "24770\n",
      "24780\n",
      "24790\n",
      "24800\n",
      "24810\n",
      "24820\n",
      "24830\n",
      "24840\n",
      "24850\n",
      "24860\n",
      "24870\n",
      "24880\n",
      "24890\n",
      "24900\n",
      "24910\n",
      "24920\n",
      "24930\n",
      "24940\n",
      "24950\n",
      "24960\n",
      "24970\n",
      "24980\n",
      "24990\n",
      "25000\n",
      "25010\n",
      "25020\n",
      "25030\n",
      "25040\n",
      "25050\n",
      "25060\n",
      "25070\n",
      "25080\n",
      "25090\n",
      "25100\n",
      "25110\n",
      "25120\n",
      "25130\n",
      "25140\n",
      "25150\n",
      "25160\n",
      "25170\n",
      "25180\n",
      "25190\n",
      "25200\n",
      "25210\n",
      "25220\n",
      "25230\n",
      "25240\n",
      "25250\n",
      "25260\n",
      "25270\n",
      "25280\n",
      "25290\n",
      "25300\n",
      "25310\n",
      "25320\n",
      "25330\n",
      "25340\n",
      "25350\n",
      "25360\n",
      "25370\n",
      "25380\n",
      "25390\n",
      "25400\n",
      "25410\n",
      "25420\n",
      "25430\n",
      "25440\n",
      "25450\n",
      "25460\n",
      "25470\n",
      "25480\n",
      "25490\n",
      "25500\n",
      "25510\n",
      "25520\n",
      "25530\n",
      "25540\n",
      "25550\n",
      "25560\n",
      "25570\n",
      "25580\n",
      "25590\n",
      "25600\n",
      "25610\n",
      "25620\n",
      "25630\n",
      "25640\n",
      "25650\n",
      "25660\n",
      "25670\n",
      "25680\n",
      "25690\n",
      "25700\n",
      "25710\n",
      "25720\n",
      "25730\n",
      "25740\n",
      "25750\n",
      "25760\n",
      "25770\n",
      "25780\n",
      "25790\n",
      "25800\n",
      "25810\n",
      "25820\n",
      "25830\n",
      "25840\n",
      "25850\n",
      "25860\n",
      "25870\n",
      "25880\n",
      "25890\n",
      "25900\n",
      "25910\n",
      "25920\n",
      "25930\n",
      "25940\n",
      "25950\n",
      "25960\n",
      "25970\n",
      "25980\n",
      "25990\n",
      "26000\n",
      "26010\n",
      "26020\n",
      "26030\n",
      "26040\n",
      "26050\n",
      "26060\n",
      "26070\n",
      "26080\n",
      "26090\n",
      "26100\n",
      "26110\n",
      "26120\n",
      "26130\n",
      "26140\n",
      "26150\n",
      "26160\n",
      "26170\n",
      "26180\n",
      "26190\n",
      "26200\n",
      "26210\n",
      "26220\n",
      "26230\n",
      "26240\n",
      "26250\n",
      "26260\n",
      "26270\n",
      "26280\n",
      "26290\n",
      "26300\n",
      "26310\n",
      "26320\n",
      "26330\n",
      "26340\n",
      "26350\n",
      "26360\n",
      "26370\n",
      "26380\n",
      "26390\n",
      "26400\n",
      "26410\n",
      "26420\n",
      "26430\n",
      "26440\n",
      "26450\n",
      "26460\n",
      "26470\n",
      "26480\n",
      "26490\n",
      "26500\n",
      "26510\n",
      "26520\n",
      "26530\n",
      "26540\n",
      "26550\n",
      "26560\n",
      "26570\n",
      "26580\n",
      "26590\n",
      "26600\n",
      "26610\n",
      "26620\n",
      "26630\n",
      "26640\n",
      "26650\n",
      "26660\n",
      "26670\n",
      "26680\n",
      "26690\n",
      "26700\n",
      "26710\n",
      "26720\n",
      "26730\n",
      "26740\n",
      "26750\n",
      "26760\n",
      "26770\n",
      "26780\n",
      "26790\n",
      "26800\n",
      "26810\n",
      "26820\n",
      "26830\n",
      "26840\n",
      "26850\n",
      "26860\n",
      "26870\n",
      "26880\n",
      "26890\n",
      "26900\n",
      "26910\n",
      "26920\n",
      "26930\n",
      "26940\n",
      "26950\n",
      "26960\n",
      "26970\n",
      "26980\n",
      "26990\n",
      "27000\n",
      "27010\n",
      "27020\n",
      "27030\n",
      "27040\n",
      "27050\n",
      "27060\n",
      "27070\n",
      "27080\n",
      "27090\n",
      "27100\n",
      "27110\n",
      "27120\n",
      "27130\n",
      "27140\n",
      "27150\n",
      "27160\n",
      "27170\n",
      "27180\n",
      "27190\n",
      "27200\n",
      "27210\n",
      "27220\n",
      "27230\n",
      "27240\n",
      "27250\n",
      "27260\n",
      "27270\n",
      "27280\n",
      "27290\n",
      "27300\n",
      "27310\n",
      "27320\n",
      "27330\n",
      "27340\n",
      "27350\n",
      "27360\n",
      "27370\n",
      "27380\n",
      "27390\n",
      "27400\n",
      "27410\n",
      "27420\n",
      "27430\n",
      "27440\n",
      "27450\n",
      "27460\n",
      "27470\n",
      "27480\n",
      "27490\n",
      "27500\n",
      "27510\n",
      "27520\n",
      "27530\n",
      "27540\n",
      "27550\n",
      "27560\n",
      "27570\n",
      "27580\n",
      "27590\n",
      "27600\n",
      "27610\n",
      "27620\n",
      "27630\n",
      "27640\n",
      "27650\n",
      "27660\n",
      "27670\n",
      "27680\n",
      "27690\n",
      "27700\n",
      "27710\n",
      "27720\n",
      "27730\n",
      "27740\n",
      "27750\n",
      "27760\n",
      "27770\n",
      "27780\n",
      "27790\n",
      "27800\n",
      "27810\n",
      "27820\n",
      "27830\n",
      "27840\n",
      "27850\n",
      "27860\n",
      "27870\n",
      "27880\n",
      "27890\n",
      "27900\n",
      "27910\n",
      "27920\n",
      "27930\n",
      "27940\n",
      "27950\n",
      "27960\n",
      "27970\n",
      "27980\n",
      "27990\n",
      "28000\n",
      "28010\n",
      "28020\n",
      "28030\n",
      "28040\n",
      "28050\n",
      "28060\n",
      "28070\n",
      "28080\n",
      "28090\n",
      "28100\n",
      "28110\n",
      "28120\n",
      "28130\n"
     ]
    }
   ],
   "source": [
    "for idx, entry in enumerate(authored_by_df.iterrows()):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    feature_vector_df.loc[entry[1][\"PaperID\"], entry[1][\"AuthorID\"]] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "      145944053 1689198 1737505 1740591 2982295 1828610 2830546 49664054  \\\n0             1       1       1     NaN     NaN     NaN     NaN      NaN   \n1           NaN     NaN     NaN       1       1     NaN     NaN      NaN   \n2           NaN     NaN     NaN     NaN     NaN       1       1        1   \n3           NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n4           NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n...         ...     ...     ...     ...     ...     ...     ...      ...   \n10408       NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n10409       NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n10410       NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n10411       NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n10412       NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n\n      1706958 1714335  ... 47013198 4777285 93882234 1731709 48044592  \\\n0         NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n1         NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n2         NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n3           1       1  ...      NaN     NaN      NaN     NaN      NaN   \n4         NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n...       ...     ...  ...      ...     ...      ...     ...      ...   \n10408     NaN     NaN  ...      NaN     NaN      NaN       1        1   \n10409     NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n10410     NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n10411     NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n10412     NaN     NaN  ...      NaN     NaN      NaN     NaN      NaN   \n\n      69451391 3180815 1784705 2138026 2833202  \n0          NaN     NaN     NaN     NaN     NaN  \n1          NaN     NaN     NaN     NaN     NaN  \n2          NaN     NaN     NaN     NaN     NaN  \n3          NaN     NaN     NaN     NaN     NaN  \n4          NaN     NaN     NaN     NaN     NaN  \n...        ...     ...     ...     ...     ...  \n10408      NaN     NaN     NaN     NaN     NaN  \n10409        1     NaN     NaN     NaN     NaN  \n10410      NaN       1     NaN     NaN     NaN  \n10411      NaN     NaN       1       1       1  \n10412      NaN     NaN     NaN     NaN     NaN  \n\n[10413 rows x 17544 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>145944053</th>\n      <th>1689198</th>\n      <th>1737505</th>\n      <th>1740591</th>\n      <th>2982295</th>\n      <th>1828610</th>\n      <th>2830546</th>\n      <th>49664054</th>\n      <th>1706958</th>\n      <th>1714335</th>\n      <th>...</th>\n      <th>47013198</th>\n      <th>4777285</th>\n      <th>93882234</th>\n      <th>1731709</th>\n      <th>48044592</th>\n      <th>69451391</th>\n      <th>3180815</th>\n      <th>1784705</th>\n      <th>2138026</th>\n      <th>2833202</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10408</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10409</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10410</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10411</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10412</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10413 rows  17544 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "1795"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"Select p.PaperID as PaperID from Papers p where p.Leaf=False and p.Pub_Year < 2013\")\n",
    "len(cursor.fetchall())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 5\n",
    "\n",
    "# Labels for each node (in this example, each node has multiple labels)\n",
    "labels = [\n",
    "    ['Author 1', 'Author 3', 'Author 7'],\n",
    "    ['Author 1', 'Author 2', 'Author 4'],\n",
    "    ['Author 2', 'Author 3', 'Author 6'],\n",
    "    ['Author 4', 'Author 5', 'Author 8'],\n",
    "    ['Author 5', 'Author 6', 'Author 8']\n",
    "]\n",
    "\n",
    "# Create a pandas DataFrame with a column for each author\n",
    "df = pd.DataFrame(columns=np.unique(np.concatenate(labels)))\n",
    "\n",
    "# Set the entries corresponding to the labels to 1\n",
    "#for i, node_labels in enumerate(labels):\n",
    "#    df.loc[i, node_labels] = 1\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "multi_label_vector = df.to_numpy()\n",
    "\n",
    "print(multi_label_vector)  # Output: array([[1., 0., 0., 1., 0., 0., 1., 0.],\n",
    "                         #                [1., 1., 0., 1., 0., 0., 0., 0.],\n",
    "                         #                [0., 1., 0., 0., 0., 1., 0., 0.],\n",
    "                         #                [0., 0., 0., 0., 1., 0., 0., 1.],\n",
    "                         #                [0., 0., 0., 0., 1., 1., 0., 1.]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "random.seed(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"./data2.bin\"):\n",
    "    added_labels = []\n",
    "    cursor.execute(\"select a.AuthorID from Authors a\")\n",
    "    all_authors = cursor.fetchall()\n",
    "    all_authors_index = [(x, all_authors[x][0]) for x in range(0, len(all_authors))]\n",
    "    authors = pd.DataFrame(all_authors_index, columns=[\"key\", \"AuthorID\"])\n",
    "    max_id = authors[\"key\"].max()\n",
    "    complete_references_raw = pd.DataFrame({'key_x' : [],  \"key_y\" : [], \"year\": [], \"PaperID\": []})\n",
    "    conc_list = []\n",
    "    # Fetch data per year (otherwise it would crash)\n",
    "    for year in range(2000,2010):\n",
    "        print(year)\n",
    "        cursor.execute(f\"select a.AuthoredByID, a2.AuthoredByID, p.Pub_Year, p.PaperID from Papers p, authoredBy a, authoredBy a2, referencedBy b where p.PaperID = b.ReferencedByID and a.AuthoredByID != 0 and p.PaperID = a.PaperID and a2.AuthoredByID != 0 and p.Pub_Year = {year} and b.ReferenceID = a2.PaperID\")\n",
    "        entries = cursor.fetchall()\n",
    "        entries_df = pd.DataFrame(entries, columns=[\"A\", \"B\", \"year\", \"PaperID\"])\n",
    "        temp = pd.merge(entries_df, authors, left_on=\"B\", right_on=\"AuthorID\")\n",
    "        all_references = pd.merge(authors, temp, left_on=\"AuthorID\", right_on=\"A\")\n",
    "        subset_all_references = all_references[[\"key_x\", \"key_y\", \"year\", \"PaperID\"]]\n",
    "        complete_references_raw = pd.concat([complete_references_raw, subset_all_references])\n",
    "\n",
    "    # Choose latest papers with existing authors in the training as validation\n",
    "    aggregated_references = complete_references_raw.groupby([\"PaperID\", \"key_x\", \"year\"], as_index=False).count()\n",
    "    author_count = aggregated_references.groupby([\"key_x\"]).size().reset_index(name='count')\n",
    "    author_count = author_count[author_count[\"count\"] != 1]\n",
    "    aggregated_references = pd.merge(aggregated_references, author_count, on=\"key_x\")\n",
    "    validation_subset_sample = random.sample(range(len(author_count)), int(val_split*len(author_count)))\n",
    "    for val_id in validation_subset_sample:\n",
    "        # Get latest paper of that author\n",
    "        latest_paper_information = aggregated_references[aggregated_references[\"key_x\"] == author_count.iloc[val_id][\"key_x\"]].sort_values(\"year\").iloc[-1]\n",
    "        val_author_id = latest_paper_information[\"key_x\"]\n",
    "        val_paper_id = latest_paper_information[\"PaperID\"]\n",
    "        val_author_label_id = authors[authors[\"key\"] == val_author_id][\"AuthorID\"].values[0]\n",
    "        added_labels.append(val_author_id)\n",
    "        # Remap the id\n",
    "        max_id += 1\n",
    "        authors = pd.concat([authors, pd.DataFrame({\"key\": [max_id], \"AuthorID\": [val_author_label_id]})])\n",
    "        aggregated_references.loc[(aggregated_references[\"PaperID\"] == val_paper_id) & (aggregated_references[\"key_x\"] == val_author_id), [\"key_x\"]] \\\n",
    "            = max_id\n",
    "        aggregated_references.drop(aggregated_references.loc[(aggregated_references[\"PaperID\"] == val_paper_id) & (aggregated_references[\"key_x\"] != max_id)].index, inplace=True)\n",
    "\n",
    "\n",
    "    complete_references_raw = complete_references_raw.groupby([\"key_x\", \"key_y\"]).size().reset_index(name='C')\n",
    "    complete_references_raw = complete_references_raw.rename(columns={\"key_x\":\"Author\", \"key_y\":\"RefAuthor\", \"C\":\"Count\"})\n",
    "    dataset = AuthorDataset(authors, complete_references_raw)\n",
    "    graph = dataset[0]\n",
    "    save_graphs(\"./data.bin\", [graph])\n",
    "else: # Load graph if exists\n",
    "    glist, label_dict = load_graphs(\"./data.bin\")\n",
    "    graph = glist[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        key_x\n",
      "190  450237.0\n"
     ]
    }
   ],
   "source": [
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "aggregated_references.drop(aggregated_references.loc[(aggregated_references[\"PaperID\"] == val_paper_id) & (aggregated_references[\"key_x\"] != max_id)].index, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "Int64Index([], dtype='int64')"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_references.loc[(aggregated_references[\"PaperID\"] == val_paper_id) & (aggregated_references[\"key_x\"] != max_id)].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Properties"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 733726\n",
      "Number of edges: 25392667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {graph.num_nodes()}\")\n",
    "print(f\"Number of edges: {graph.num_edges()}\")\n",
    "in_degrees = graph.in_degrees().numpy()\n",
    "out_degrees = graph.out_degrees().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(in_degrees[in_degrees != 0], range=(0,500))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(out_degrees[out_degrees != 0], range=(0,50), bins=50)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "node_subset = random.sample(range(graph.num_nodes()), 4000)\n",
    "sub_graph = graph.subgraph(node_subset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(in_feats, hid_feats, allow_zero_in_degree=True, norm='none', weight=True, bias=True)\n",
    "        self.conv2 = GraphConv(hid_feats, out_feats, allow_zero_in_degree=True, norm='none', weight=True, bias=True)\n",
    "\n",
    "    def forward(self, graph, inputs, edge_weight):\n",
    "        norm = EdgeWeightNorm(norm='both')\n",
    "        norm_edge_weight = norm(graph, edge_weight)\n",
    "        #print(norm_edge_weight)\n",
    "        h = self.conv1(graph, inputs, edge_weight=norm_edge_weight)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h, edge_weight=norm_edge_weight)\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "node_features = graph.ndata['feat'].float()\n",
    "node_labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "valid_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "edge_weights = graph.edata['weight'].float()\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item() + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, edge_weights, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features, edge_weights)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        print(indices)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "29100277"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2153415372304 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [51]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# forward propagation by using all nodes\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# compute loss\u001B[39;00m\n\u001B[1;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits[train_mask], node_labels[train_mask]\u001B[38;5;241m.\u001B[39mlong())\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [46]\u001B[0m, in \u001B[0;36mGCN.forward\u001B[0;34m(self, graph, inputs, edge_weight)\u001B[0m\n\u001B[1;32m     12\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(graph, inputs, edge_weight\u001B[38;5;241m=\u001B[39mnorm_edge_weight)\n\u001B[1;32m     13\u001B[0m h \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h)\n\u001B[0;32m---> 14\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnorm_edge_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m h\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/nn/pytorch/conv/graphconv.py:431\u001B[0m, in \u001B[0;36mGraphConv.forward\u001B[0;34m(self, graph, feat, weight, edge_weight)\u001B[0m\n\u001B[1;32m    429\u001B[0m     rst \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mdstdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 431\u001B[0m         rst \u001B[38;5;241m=\u001B[39m \u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_norm \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m    434\u001B[0m     degs \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39min_degrees()\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2153415372304 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "model = GCN(in_feats=n_features, hid_feats=12, out_feats=n_labels)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes\n",
    "    logits = model(graph, node_features, edge_weights)\n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask].long())\n",
    "    # compute validation accuracy\n",
    "    acc = evaluate(model, graph, node_features, edge_weights, node_labels, valid_mask)\n",
    "    print(acc)\n",
    "    # backward propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n",
    "\n",
    "    # Save model if necessary.  Omitted in this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [114]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m sample_g \u001B[38;5;241m=\u001B[39m dgl\u001B[38;5;241m.\u001B[39mgraph(([\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m],[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m9\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m11\u001B[39m, \u001B[38;5;241m11\u001B[39m]))\n\u001B[1;32m      2\u001B[0m sample_g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeat\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m sample_g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#sample_g.edata['weights'] = torch.tensor([2,2,1,1,1,3,1,1,1,3,1,1,1,1,1,1,1,1]).type(torch.float32)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m sample_g\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3\u001B[39m])\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "sample_g = dgl.graph(([1, 2, 2, 1, 1, 3, 3, 2, 3, 2, 5, 1, 2, 2, 1, 3, 2, 5],[0, 0, 1, 3, 4, 4, 5, 6, 6, 7, 7, 8, 8, 9, 10, 10, 11, 11]))\n",
    "sample_g.ndata['feat'] = torch.randn(12, 1)\n",
    "sample_g.ndata['label'] = torch.tensor(list(range(8)) + [0, 7,1, 4, 7])\n",
    "#sample_g.edata['weights'] = torch.tensor([2,2,1,1,1,3,1,1,1,3,1,1,1,1,1,1,1,1]).type(torch.float32)\n",
    "sample_g.edata['weights'] = torch.tensor([2,2,1,1,1,3,1,1,3,3,1,3,3,3,3,3,3,3]).type(torch.float32)\n",
    "t_mask = torch.zeros(12, dtype=torch.bool)\n",
    "v_mask = torch.zeros(12, dtype=torch.bool)\n",
    "t_mask[:8] = True\n",
    "v_mask[8:] = True\n",
    "sample_g.ndata['train_mask'] = t_mask\n",
    "sample_g.ndata['val_mask'] = v_mask\n",
    "sample_g.ndata['test_mask'] = v_mask\n",
    "#sample_g = dgl.add_self_loop(sample_g)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "class BinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return self.loss_fn(logits, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sample_g = dgl.graph(([2,2,3,3,2,3,6],[0,1,1,4,5,5,0]))\n",
    "sample_g.ndata['feat'] = torch.randn(7, 2)\n",
    "sample_g.ndata['label'] = torch.tensor([[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,1,0],[0,0,0,0,0,1],[0,1,0,0,0,0]])\n",
    "t_mask = torch.zeros(7, dtype=torch.bool)\n",
    "v_mask = torch.zeros(7, dtype=torch.bool)\n",
    "t_mask[:4] = True\n",
    "v_mask[5:] = True\n",
    "sample_g.ndata['train_mask'] = t_mask\n",
    "sample_g.ndata['val_mask'] = v_mask\n",
    "sample_g.ndata['test_mask'] = v_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.2875,  1.3238],\n        [ 1.4970,  0.2341],\n        [ 1.3481,  0.1165],\n        [-0.5130, -0.4312],\n        [ 0.8833, -2.4597],\n        [-2.0096,  0.0357],\n        [-0.1836, -0.7578]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_g.ndata['feat']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "graph2 = sample_g"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(sample_g.to_networkx())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "node_features = sample_g.ndata['feat']\n",
    "node_labels = sample_g.ndata['label']\n",
    "train_mask = sample_g.ndata['train_mask']\n",
    "valid_mask = sample_g.ndata['val_mask']\n",
    "#test_mask = sample_g.ndata['test_mask']\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item() + 1)\n",
    "#sample_g = dgl.add_self_loop(sample_g)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Number of categories: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([False, False, False,  ..., False, False, False])"
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = dgl.data.CiteseerGraphDataset()\n",
    "print('Number of categories:', dataset2.num_classes)\n",
    "graph2 = dataset2[0]\n",
    "graph2.ndata['val_mask']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        #self.conv1 = dglnn.SAGEConv(\n",
    "        #    in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
    "        #self.conv2 = dglnn.SAGEConv(\n",
    "        #    in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
    "        self.conv1 = GraphConv(in_feats, hid_feats, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(hid_feats, out_feats, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        norm = EdgeWeightNorm(norm='both')\n",
    "        #norm_edge_weight = norm(graph, edge_weight)\n",
    "        #print(norm_edge_weight)\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "\n",
    "# Create the model with given dimensions\n",
    "#model = GCN(graph2.ndata['feat'].shape[1], 64, dataset2.num_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'weights'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [280]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mgraph2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweights\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mdtype\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/view.py:192\u001B[0m, in \u001B[0;36mHeteroEdgeDataView.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_e_repr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_etid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_edges\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/frame.py:622\u001B[0m, in \u001B[0;36mFrame.__getitem__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[1;32m    610\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the column of the given name.\u001B[39;00m\n\u001B[1;32m    611\u001B[0m \n\u001B[1;32m    612\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    620\u001B[0m \u001B[38;5;124;03m        Column data.\u001B[39;00m\n\u001B[1;32m    621\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_columns\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mdata\n",
      "\u001B[0;31mKeyError\u001B[0m: 'weights'"
     ]
    }
   ],
   "source": [
    "graph2.edata['weights'].dtype"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.6948, train_acc=0.4688, val_acc=0.5625\n",
      "Epoch 10: loss=0.6682, train_acc=0.6562, val_acc=0.6250\n",
      "Epoch 20: loss=0.6434, train_acc=0.6719, val_acc=0.6250\n",
      "Epoch 30: loss=0.6195, train_acc=0.6719, val_acc=0.6250\n",
      "Epoch 40: loss=0.5971, train_acc=0.7031, val_acc=0.7500\n",
      "Epoch 50: loss=0.5777, train_acc=0.7188, val_acc=0.8125\n",
      "Epoch 60: loss=0.5610, train_acc=0.7188, val_acc=0.8125\n",
      "Epoch 70: loss=0.5455, train_acc=0.7812, val_acc=0.8125\n",
      "Epoch 80: loss=0.5312, train_acc=0.7969, val_acc=0.8125\n",
      "Epoch 90: loss=0.5183, train_acc=0.7969, val_acc=0.8125\n",
      "Epoch 100: loss=0.5066, train_acc=0.7969, val_acc=0.8125\n",
      "Epoch 110: loss=0.4961, train_acc=0.7969, val_acc=0.8125\n",
      "Epoch 120: loss=0.4863, train_acc=0.7969, val_acc=0.8125\n",
      "Epoch 130: loss=0.4772, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 140: loss=0.4687, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 150: loss=0.4609, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 160: loss=0.4537, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 170: loss=0.4473, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 180: loss=0.4417, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 190: loss=0.4367, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 200: loss=0.4321, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 210: loss=0.4280, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 220: loss=0.4242, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 230: loss=0.4207, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 240: loss=0.4173, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 250: loss=0.4142, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 260: loss=0.4112, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 270: loss=0.4082, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 280: loss=0.4054, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 290: loss=0.4027, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 300: loss=0.4000, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 310: loss=0.3976, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 320: loss=0.3953, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 330: loss=0.3932, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 340: loss=0.3912, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 350: loss=0.3894, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 360: loss=0.3878, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 370: loss=0.3862, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 380: loss=0.3849, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 390: loss=0.3836, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 400: loss=0.3825, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 410: loss=0.3814, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 420: loss=0.3804, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 430: loss=0.3795, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 440: loss=0.3787, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 450: loss=0.3779, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 460: loss=0.3771, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 470: loss=0.3764, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 480: loss=0.3758, train_acc=0.8125, val_acc=0.8750\n",
      "Epoch 490: loss=0.3752, train_acc=0.8125, val_acc=0.8750\n"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    #test_mask = g.ndata['test_mask']\n",
    "    loss_fn = BinaryCrossEntropyLoss()\n",
    "    for e in range(500):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "        # Compute prediction\n",
    "        pred = logits\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.binary_cross_entropy_with_logits(logits[train_mask], labels[train_mask].float())\n",
    "        # Compute accuracy on training/validation/test\n",
    "        #train_acc = (pred[train_mask].long() == labels[train_mask]).float().mean()\n",
    "        train_acc = (logits[train_mask] > 0).long().eq(labels[train_mask]).float().mean()\n",
    "        #val_acc = (pred[val_mask].long() == labels[val_mask]).float().mean()\n",
    "        val_acc = (logits[g.ndata['val_mask']] > 0).long().eq(g.ndata['label'][g.ndata['val_mask']]).float().mean()\n",
    "        #test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "        test_acc = 0\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            print(f'Epoch {e}: loss={loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "model = GCN(2, 8, 8)\n",
    "train(g, model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "g = graph2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(graph2.to_networkx())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# Create the graph and node features\n",
    "g = dgl.DGLGraph()\n",
    "num_papers = 400\n",
    "num_authors = 200\n",
    "g.add_nodes(num_papers)\n",
    "#features = torch.randn(num_papers, 2)\n",
    "features = torch.zeros(num_papers, 0)\n",
    "# Define the labels for each node\n",
    "labels = torch.randint(2, size=(num_papers, num_authors))\n",
    "g.ndata['label'] = labels\n",
    "\n",
    "# Add random edges to the graph\n",
    "for i in range(num_papers):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    for j in range(i+1, num_papers):\n",
    "        if random.random() < 0.1:  # 50% chance of adding an edge\n",
    "            g.add_edge(j, i)\n",
    "\n",
    "# Add the nodes and edges to the graph\n",
    "#g.add_nodes(len(features))\n",
    "g.ndata['feat'] = features\n",
    "g.ndata['label'] = labels\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "t_mask = torch.zeros(len(features), dtype=torch.bool)\n",
    "v_mask = torch.zeros(len(features), dtype=torch.bool)\n",
    "t_mask[:25000] = True\n",
    "v_mask[25000:] = True\n",
    "g.ndata[\"train_mask\"] = t_mask\n",
    "g.ndata[\"val_mask\"] = v_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.6765, -0.7956],\n        [-0.2571, -1.2591],\n        [-1.1323, -0.1018],\n        ...,\n        [-1.5830, -0.5672],\n        [-1.1757, -0.5143],\n        [-1.0874, -1.0603]])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7fa0c9389240> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nx.draw(g.to_networkx())\n",
    "g.ndata['feat']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]])"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['feat']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "[15:14:18] /opt/dgl/src/array/./check.h:38: Check failed: arrays[i].IsContiguous(): Expect U_data to be a contiguous tensor\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x7fa0ecfa16ef]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::aten::CheckContiguous(std::vector<dgl::runtime::NDArray, std::allocator<dgl::runtime::NDArray> > const&, std::vector<std::string, std::allocator<std::string> > const&)+0x127) [0x7fa0ed222d07]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(+0x419d7b) [0x7fa0ed219d7b]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(+0x41a891) [0x7fa0ed21a891]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(DGLFuncCall+0x48) [0x7fa0ed269118]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/dgl/_ffi/_cy3/core.cpython-310-x86_64-linux-gnu.so(+0x14fa3) [0x7fa0eca14fa3]\n  [bt] (6) /usr/local/lib/python3.10/dist-packages/dgl/_ffi/_cy3/core.cpython-310-x86_64-linux-gnu.so(+0x1578b) [0x7fa0eca1578b]\n  [bt] (7) /home/david/Work/TinyML/image_labeler_application/venv/local/bin/python(_PyObject_MakeTpCall+0x25b) [0x556b685797db]\n  [bt] (8) /home/david/Work/TinyML/image_labeler_application/venv/local/bin/python(_PyEval_EvalFrameDefault+0x64f9) [0x556b68571e39]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mDGLError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [113]\u001B[0m, in \u001B[0;36m<cell line: 40>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m500\u001B[39m):\n\u001B[0;32m---> 41\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfeat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(logits[g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]], g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m     43\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [113]\u001B[0m, in \u001B[0;36mGraphClassificationModel.forward\u001B[0;34m(self, g, features)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m     18\u001B[0m     h \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m---> 19\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m h\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/nn/pytorch/conv/graphconv.py:428\u001B[0m, in \u001B[0;36mGraphConv.forward\u001B[0;34m(self, graph, feat, weight, edge_weight)\u001B[0m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    426\u001B[0m     \u001B[38;5;66;03m# aggregate first then mult W\u001B[39;00m\n\u001B[1;32m    427\u001B[0m     graph\u001B[38;5;241m.\u001B[39msrcdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m feat_src\n\u001B[0;32m--> 428\u001B[0m     \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43maggregate_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mh\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    429\u001B[0m     rst \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mdstdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:4895\u001B[0m, in \u001B[0;36mDGLHeteroGraph.update_all\u001B[0;34m(self, message_func, reduce_func, apply_node_func, etype)\u001B[0m\n\u001B[1;32m   4893\u001B[0m _, dtid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mmetagraph\u001B[38;5;241m.\u001B[39mfind_edge(etid)\n\u001B[1;32m   4894\u001B[0m g \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m etype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m[etype]\n\u001B[0;32m-> 4895\u001B[0m ndata \u001B[38;5;241m=\u001B[39m \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessage_passing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessage_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapply_node_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4896\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m core\u001B[38;5;241m.\u001B[39mis_builtin(reduce_func) \u001B[38;5;129;01mand\u001B[39;00m reduce_func\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m ndata:\n\u001B[1;32m   4897\u001B[0m     \u001B[38;5;66;03m# Replace infinity with zero for isolated nodes\u001B[39;00m\n\u001B[1;32m   4898\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ndata\u001B[38;5;241m.\u001B[39mkeys())[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/core.py:357\u001B[0m, in \u001B[0;36mmessage_passing\u001B[0;34m(g, mfunc, rfunc, afunc)\u001B[0m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"Invoke message passing computation on the whole graph.\u001B[39;00m\n\u001B[1;32m    337\u001B[0m \n\u001B[1;32m    338\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;124;03m    Results from the message passing computation.\u001B[39;00m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (is_builtin(mfunc) \u001B[38;5;129;01mand\u001B[39;00m is_builtin(rfunc) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    355\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(ops, \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(mfunc\u001B[38;5;241m.\u001B[39mname, rfunc\u001B[38;5;241m.\u001B[39mname), \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    356\u001B[0m     \u001B[38;5;66;03m# invoke fused message passing\u001B[39;00m\n\u001B[0;32m--> 357\u001B[0m     ndata \u001B[38;5;241m=\u001B[39m \u001B[43minvoke_gspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    359\u001B[0m     \u001B[38;5;66;03m# invoke message passing in two separate steps\u001B[39;00m\n\u001B[1;32m    360\u001B[0m     \u001B[38;5;66;03m# message phase\u001B[39;00m\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_builtin(mfunc):\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/core.py:332\u001B[0m, in \u001B[0;36minvoke_gspmm\u001B[0;34m(graph, mfunc, rfunc, srcdata, dstdata, edata)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m: \u001B[38;5;66;03m# \"copy_e\"\u001B[39;00m\n\u001B[1;32m    331\u001B[0m             x \u001B[38;5;241m=\u001B[39m data_dict_to_list(graph, x, mfunc, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124me\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 332\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {rfunc\u001B[38;5;241m.\u001B[39mout_field : z}\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/ops/spmm.py:189\u001B[0m, in \u001B[0;36m_gen_copy_reduce_func.<locals>.func\u001B[0;34m(g, x)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(g, x):\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary_op \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcopy_u\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 189\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcopy_lhs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m gspmm(g, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcopy_rhs\u001B[39m\u001B[38;5;124m'\u001B[39m, reduce_op, \u001B[38;5;28;01mNone\u001B[39;00m, x)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/ops/spmm.py:75\u001B[0m, in \u001B[0;36mgspmm\u001B[0;34m(g, op, reduce_op, lhs_data, rhs_data)\u001B[0m\n\u001B[1;32m     73\u001B[0m         lhs_data, rhs_data \u001B[38;5;241m=\u001B[39m reshape_lhs_rhs(lhs_data, rhs_data)\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# With max and min reducers infinity will be returned for zero degree nodes\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mgspmm_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m                         \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mlhs_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrhs_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# lhs_data or rhs_data is None only in unary functions like ``copy-u`` or ``copy_e``\u001B[39;00m\n\u001B[1;32m     80\u001B[0m     lhs_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m g\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mnumber_of_ntypes() \u001B[38;5;28;01mif\u001B[39;00m lhs_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m lhs_data\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:724\u001B[0m, in \u001B[0;36mgspmm\u001B[0;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001B[0m\n\u001B[1;32m    722\u001B[0m     op \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmul\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    723\u001B[0m     rhs_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m/\u001B[39m rhs_data\n\u001B[0;32m--> 724\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGSpMM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlhs_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrhs_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/autocast_mode.py:105\u001B[0m, in \u001B[0;36mcustom_fwd.<locals>.decorate_fwd\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fwd(\u001B[38;5;241m*\u001B[39m_cast(args, cast_inputs), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_cast(kwargs, cast_inputs))\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfwd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:106\u001B[0m, in \u001B[0;36mGSpMM.forward\u001B[0;34m(ctx, gidx, op, reduce_op, X, Y)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;129m@custom_fwd\u001B[39m(cast_inputs\u001B[38;5;241m=\u001B[39mth\u001B[38;5;241m.\u001B[39mfloat16)\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(ctx, gidx, op, reduce_op, X, Y):\n\u001B[0;32m--> 106\u001B[0m     out, (argX, argY) \u001B[38;5;241m=\u001B[39m \u001B[43m_gspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     reduce_last \u001B[38;5;241m=\u001B[39m _need_reduce_last_dim(X, Y)\n\u001B[1;32m    108\u001B[0m     X_shape \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/dgl/sparse.py:228\u001B[0m, in \u001B[0;36m_gspmm\u001B[0;34m(gidx, op, reduce_op, u, e)\u001B[0m\n\u001B[1;32m    226\u001B[0m arg_e_nd \u001B[38;5;241m=\u001B[39m to_dgl_nd_for_write(arg_e)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gidx\u001B[38;5;241m.\u001B[39mnumber_of_edges(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 228\u001B[0m     \u001B[43m_CAPI_DGLKernelSpMM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_u\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_e\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mto_dgl_nd_for_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m                        \u001B[49m\u001B[43marg_u_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[43m                        \u001B[49m\u001B[43marg_e_nd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;66;03m# NOTE(zihao): actually we can avoid the following step, because arg_*_nd\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;66;03m# refers to the data that stores arg_*. After we call _CAPI_DGLKernelSpMM,\u001B[39;00m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;66;03m# arg_* should have already been changed. But we found this doesn't work\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;66;03m# The workaround is proposed by Jinjing, and we still need to investigate\u001B[39;00m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;66;03m# where the problem is.\u001B[39;00m\n\u001B[1;32m    241\u001B[0m arg_u \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m arg_u \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m F\u001B[38;5;241m.\u001B[39mzerocopy_from_dgl_ndarray(arg_u_nd)\n",
      "File \u001B[0;32mdgl/_ffi/_cython/./function.pxi:293\u001B[0m, in \u001B[0;36mdgl._ffi._cy3.core.FunctionBase.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mdgl/_ffi/_cython/./function.pxi:239\u001B[0m, in \u001B[0;36mdgl._ffi._cy3.core.FuncCall\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mDGLError\u001B[0m: [15:14:18] /opt/dgl/src/array/./check.h:38: Check failed: arrays[i].IsContiguous(): Expect U_data to be a contiguous tensor\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x7fa0ecfa16ef]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::aten::CheckContiguous(std::vector<dgl::runtime::NDArray, std::allocator<dgl::runtime::NDArray> > const&, std::vector<std::string, std::allocator<std::string> > const&)+0x127) [0x7fa0ed222d07]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(+0x419d7b) [0x7fa0ed219d7b]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(+0x41a891) [0x7fa0ed21a891]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(DGLFuncCall+0x48) [0x7fa0ed269118]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/dgl/_ffi/_cy3/core.cpython-310-x86_64-linux-gnu.so(+0x14fa3) [0x7fa0eca14fa3]\n  [bt] (6) /usr/local/lib/python3.10/dist-packages/dgl/_ffi/_cy3/core.cpython-310-x86_64-linux-gnu.so(+0x1578b) [0x7fa0eca1578b]\n  [bt] (7) /home/david/Work/TinyML/image_labeler_application/venv/local/bin/python(_PyObject_MakeTpCall+0x25b) [0x556b685797db]\n  [bt] (8) /home/david/Work/TinyML/image_labeler_application/venv/local/bin/python(_PyEval_EvalFrameDefault+0x64f9) [0x556b68571e39]\n\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class GraphClassificationModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(GraphClassificationModel, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            dgl.nn.GraphConv(in_dim, hidden_dim, allow_zero_in_degree=True),\n",
    "            dgl.nn.GraphConv(hidden_dim, hidden_dim, allow_zero_in_degree=True),\n",
    "            dgl.nn.GraphConv(hidden_dim, out_dim, allow_zero_in_degree=True)\n",
    "        ])\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = h.contiguous()\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "\n",
    "# Define the loss function\n",
    "class BinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return self.loss_fn(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and the loss function\n",
    "model = GraphClassificationModel(0, 8, 200)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    logits = model(g, g.ndata['feat'])\n",
    "    loss = F.binary_cross_entropy_with_logits(logits[g.ndata['train_mask']], g.ndata['label'][g.ndata['train_mask']].float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "    # Evaluation loop\n",
    "        with torch.no_grad():\n",
    "            logits = model(g, g.ndata['feat'])\n",
    "            train_acc = (logits[g.ndata['train_mask']] > 0).long().eq(g.ndata['label'][g.ndata['train_mask']]).float().mean()\n",
    "            val_acc = (logits[g.ndata['val_mask']] > 0).long().eq(g.ndata['label'][g.ndata['val_mask']]).float().mean()\n",
    "            print(f'Epoch {epoch}: loss={loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "# Test the model on the test set\n",
    "#test_acc = (logits[g.ndata['test_mask']] > 0).long().eq(g.ndata['label'][g.ndata['test_mask']]).float().mean()\n",
    "#print(f'Test accuracy: {test_acc:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.6765, -0.7956],\n        [-0.2571, -1.2591],\n        [-1.1323, -0.1018],\n        ...,\n        [-1.5830, -0.5672],\n        [-1.1757, -0.5143],\n        [-1.0874, -1.0603]])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['feat']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.4759, -0.2498, -5.9102, -5.0863, -6.0688, -5.4875],\n        [ 0.0931, -0.4158, -4.3912, -3.8085, -4.6465, -4.2222],\n        [-0.9526, -0.8691, -0.2411, -0.3174, -0.7606, -0.7654],\n        [-0.9526, -0.8691, -0.2411, -0.3174, -0.7606, -0.7654],\n        [-0.2132, -0.5485, -3.1756, -2.7859, -3.5083, -3.2097],\n        [ 0.0931, -0.4158, -4.3912, -3.8085, -4.6465, -4.2222],\n        [-0.9526, -0.8691, -0.2411, -0.3174, -0.7606, -0.7654]])"
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 1, 0, 1, 1, 0, 1],\n        [1, 1, 0, 0, 0, 0, 1, 0],\n        [0, 1, 1, 0, 1, 1, 0, 1],\n        [0, 0, 1, 0, 0, 1, 1, 1],\n        [0, 1, 1, 0, 0, 0, 0, 0],\n        [0, 1, 1, 0, 0, 1, 0, 1],\n        [1, 0, 0, 0, 1, 1, 1, 1],\n        [1, 0, 1, 0, 1, 0, 1, 0],\n        [1, 1, 0, 1, 0, 1, 1, 1],\n        [0, 1, 0, 1, 1, 1, 0, 0]])"
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = 10\n",
    "features = torch.randn(num_nodes, 2)\n",
    "\n",
    "# Define the labels for each node\n",
    "labels = torch.randint(2, size=(num_nodes, 8))\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
